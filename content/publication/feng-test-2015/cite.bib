@inproceedings{feng_test_2015,
 abstract = {In crowdsourced testing, users can be incentivized to perform testing tasks and report their results, and because crowdsourced workers are often paid per task, there is a financial incentive to complete tasks quickly rather than well. These reports of the crowdsourced testing tasks are called "test reports" and are composed of simple natural language and screenshots. Back at the software-development organization, developers must manually inspect the test reports to judge their value for revealing faults. Due to the nature of crowdsourced work, the number of test reports are often difficult to comprehensively inspect and process. In order to help with this daunting task, we created the first technique of its kind, to the best of our knowledge, to prioritize test reports for manual inspection. Our technique utilizes two key strategies: (1) a diversity strategy to help developers inspect a wide variety of test reports and to avoid duplicates and wasted effort on falsely classified faulty behavior, and (2) a risk strategy to help developers identify test reports that may be more likely to be fault-revealing based on past observations. Together, these strategies form our DivRisk strategy to prioritize test reports in crowd- sourced testing. Three industrial projects have been used to evaluate the effectiveness of test report prioritization methods. The results of the empirical study show that: (1) DivRisk can significantly outperform random prioritization; (2) DivRisk can approximate the best theoretical result for a real-world industrial mobile application. In addition, we provide some practical guidelines of test report prioritization for crowdsourced testing based on the empirical study and our experiences.},
 address = {New York, NY, USA},
 author = {Feng, Yang and Chen, Zhenyu and Jones, James A. and Fang, Chunrong and Xu, Baowen},
 booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
 doi = {10.1145/2786805.2786862},
 isbn = {978-1-4503-3675-8},
 keywords = {Crowdsourcing testing, natural language processing, test diversity, test report prioritization},
 month = {August},
 pages = {225--236},
 publisher = {Association for Computing Machinery},
 series = {ESEC/FSE 2015},
 title = {Test report prioritization to assist crowdsourced testing},
 url = {https://doi.org/10.1145/2786805.2786862},
 urldate = {2023-09-05},
 year = {2015}
}

