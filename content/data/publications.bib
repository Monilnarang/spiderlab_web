
@inproceedings{feng_empirical_2018,
	title = {An {Empirical} {Study} on {Software} {Failure} {Classification} with {Multi}-label and {Problem}-{Transformation} {Techniques}},
	doi = {10.1109/ICST.2018.00039},
	abstract = {Classification techniques have been used in software-engineering research to perform tasks such as categorizing software executions. Traditionally, existing work has proposed single-label failure classification techniques, in which the training and subsequent executions are labeled with a singular fault attribution. Although such approaches have received substantial attention in research on automated software engineering, in reality, recent work shows that the assumption of such a single attribution is often unrealistic: in practice, the inherent characteristics of software behavior, such as multiple faults that contribute to failures and fault interactions, may negatively influence the effectiveness of these techniques. To relax this unrealistic assumption, in the machine learning field, researchers have proposed new approaches for multi-label classification. However, the effectiveness and efficiency of such approaches varies widely based upon application domains. In this paper, we empirically investigate the performance of these new approaches on the failure classification task under different application settings. We conducted experiments using eight classification techniques on five subject programs with more than 8,000 faulty versions to investigate how each such technique accounts for the intricacies of software behavior. Our experimental results show that multi-label techniques provide improved accuracy over single-label. We also evaluated the efficiency of the training and prediction phases of each technique, and offer guidance as to the applicability for each technique for different usage contexts.},
	booktitle = {2018 {IEEE} 11th {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Feng, Yang and Jones, James A. and Chen, Zhenyu and Fang, Chunrong},
	month = apr,
	year = {2018},
	keywords = {Computer crashes, Empirical Study, Failure Classification, Machine learning, Maintenance engineering, multi-label classification, Software, Task analysis, Training, Training data},
	pages = {320--330},
	file = {IEEE Xplore Abstract Record:files/4/8367059.html:text/html},
}

@inproceedings{liu_generating_2018,
	title = {Generating {Descriptions} for {Screenshots} to {Assist} {Crowdsourced} {Testing}},
	doi = {10.1109/SANER.2018.8330246},
	abstract = {Crowdsourced software testing has been shown to be capable of detecting many bugs and simulating real usage scenarios. As such, it is popular in mobile-application testing. However in mobile testing, test reports often consist of only some screenshots and short text descriptions. Inspecting and under-standing the overwhelming number of mobile crowdsourced test reports becomes a time-consuming but inevitable task. The paucity and potential inaccuracy of textual information and the well-defined screenshots of activity views within mobile applications motivate us to propose a novel technique to assist developers in understanding crowdsourced test reports by automatically describing the screenshots. To reach this goal, in this paper, we propose a fully automatic technique to generate descriptive words for the well-defined screenshots. We employ the test reports written by professional testers to build up language models. We use the computer-vision technique, namely Spatial Pyramid Matching (SPM), to measure similarities and extract features from the screenshot images. The experimental results, based on more than 1000 test reports from 4 industrial crowdsourced projects, show that our proposed technique is promising for developers to better understand the mobile crowdsourced test reports.},
	booktitle = {2018 {IEEE} 25th {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Liu, Di and Zhang, Xiaofang and Feng, Yang and Jones, James A.},
	month = mar,
	year = {2018},
	keywords = {Task analysis, Training, Buildings, Computer bugs, Feature extraction, Mobile applications, Testing},
	pages = {492--496},
	file = {IEEE Xplore Abstract Record:files/6/8330246.html:text/html},
}

@inproceedings{feng_hierarchical_2018,
	address = {New York, NY, USA},
	series = {{ICPC} '18},
	title = {Hierarchical {Abstraction} of {Execution} {Traces} for {Program} {Comprehension}},
	isbn = {978-1-4503-5714-2},
	url = {https://doi.org/10.1145/3196321.3196343},
	doi = {10.1145/3196321.3196343},
	abstract = {Understanding the dynamic behavior of a software system is one of the most important and time-consuming tasks for today's software maintainers. In practice, understanding the inner workings of software requires studying the source code and documentation and inserting logging code in order to map high-level descriptions of the program behavior with low-level implementation, i.e., the source code. Unfortunately, for large codebases and large log files, such cognitive mapping can be quite challenging. To bridge the cognitive gap between the source code and detailed models of program behavior, we propose a fully automatic approach to present a semantic abstraction with different levels of functional granularity from full execution traces. Our approach builds multi-level abstractions and identifies frequent behaviors at each level based on a number of execution traces, and then, it labels phases within individual execution traces according to the identified major functional behaviors of the system. To validate our approach, we conducted a case study on a large-scale subject program, Javac, to demonstrate the effectiveness of the mining result. Furthermore, the results of a user study demonstrate that our approach is capable of presenting users a high-level comprehensible abstraction of execution behavior. Based on a real world subject program the participants in our user study were able to achieve a mean accuracy of 70\%.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 26th {Conference} on {Program} {Comprehension}},
	publisher = {Association for Computing Machinery},
	author = {Feng, Yang and Dreef, Kaj and Jones, James A. and van Deursen, Arie},
	month = may,
	year = {2018},
	pages = {86--96},
}

@inproceedings{servant_fuzzy_2017,
	title = {Fuzzy {Fine}-{Grained} {Code}-{History} {Analysis}},
	doi = {10.1109/ICSE.2017.74},
	abstract = {Existing software-history techniques represent source-code evolution as an absolute and unambiguous mapping of lines of code in prior revisions to lines of code in subsequent revisions. However, the true evolutionary lineage of a line of code is often complex, subjective, and ambiguous. As such, existing techniques are predisposed to, both, overestimate and underestimate true evolution lineage. In this paper, we seek to address these issues by providing a more expressive model of code evolution, the fuzzy history graph, by representing code lineage as a continuous (i.e., fuzzy) metric rather than a discrete (i.e., absolute) one. Using this more descriptive model, we additionally provide a novel multi-revision code-history analysis - fuzzy history slicing. In our experiments over three real-world software systems, we found that the fuzzy history graph provides a tunable balance of precision and recall, and an overall improved accuracy over existing code-evolution models. Furthermore, we found that the use of such a fuzzy model of history provided improved accuracy for code-history analysis tasks.},
	booktitle = {2017 {IEEE}/{ACM} 39th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Servant, Francisco and Jones, James A.},
	month = may,
	year = {2017},
	note = {ISSN: 1558-1225},
	keywords = {Computer bugs, Analytical models, Cloning, Computational modeling, computer aided software engineering, History, Measurement, reasoning about programs, software engineering, software maintenance, Solids},
	pages = {746--757},
	file = {IEEE Xplore Abstract Record:files/9/7985710.html:text/html},
}

@article{palepu_dynamic_2017,
	title = {Dynamic {Dependence} {Summaries}},
	volume = {25},
	issn = {1049-331X},
	url = {https://dl.acm.org/doi/10.1145/2968444},
	doi = {10.1145/2968444},
	abstract = {Software engineers construct modern-day software applications by building on existing software libraries and components that they necessarily do not author themselves. Thus, contemporary software applications rely heavily on existing standard and third-party libraries for their execution and behavior. As such, effective runtime analysis of such a software application’s behavior is met with new challenges. To perform dynamic analysis of a software application, all transitively dependent external libraries must also be monitored and analyzed at each layer of the software application’s call stack. However, monitoring and analyzing large and often numerous external libraries may prove to be prohibitively expensive. Moreover, an overabundance of library-level analyses may obfuscate the details of the actual software application’s dynamic behavior. In other words, the extensive use of existing libraries by a software application renders the results of its dynamic analysis both expensive to compute and difficult to understand. We model software component behavior as dynamically observed data- and control dependencies between inputs and outputs of a software component. Such data- and control dependencies are monitored at a fine-grain instruction-level and are collected as dynamic execution traces for software runs. As an approach to address the complexities and expenses associated with analyzing dynamically observable behavior of software components, we summarize and reuse the data- and control dependencies between the inputs and outputs of software components. Dynamically monitored data- and control dependencies, between the inputs and outputs of software components, upon summarization are called dynamic dependence summaries. Software components, equipped with dynamic dependence summaries, afford the omission of their exhaustive runtime analysis. Nonetheless, the reuse of dependence summaries would necessitate the abstraction of any concrete runtime information enclosed within the summary, thus potentially causing a loss in the information modeled by the dependence summary. Therefore, benefits to the efficiency of dynamic analyses that use such summarization may be afforded with losses of accuracy. As such, we evaluate the potential accuracy loss and the potential performance gain with the use of dynamic dependence summaries. Our results show, on average, a 13× speedup with the use of dynamic dependence summaries, with an accuracy of 90\% in a real-world software engineering task.},
	number = {4},
	urldate = {2023-09-05},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Palepu, Vijay Krishna and Xu, Guoqing and Jones, James A.},
	month = jan,
	year = {2017},
	keywords = {dependence analysis, Dynamic analysis, dynamic slicing, summaries},
	pages = {30:1--30:41},
	file = {Full Text PDF:files/11/Palepu et al. - 2017 - Dynamic Dependence Summaries.pdf:application/pdf},
}

@inproceedings{feng_multi-objective_2016,
	title = {Multi-objective {Test} {Report} {Prioritization} {Using} {Image} {Understanding}},
	abstract = {In crowdsourced software testing, inspecting the large number of test reports is an overwhelming but inevitable software maintenance task. In recent years, to alleviate this task, many text-based test-report classification and prioritization techniques have been proposed. However in the mobile testing domain, test reports often consist of more screenshots and shorter descriptive text, and thus text-based techniques may be ineffective or inapplicable. The shortage and ambiguity of natural-language text information and the well defined screenshots of activity views within mobile applications motivate our novel technique based on using image understanding for multi-objective test-report prioritization. In this paper, by taking the similarity of screenshots into consideration, we present a multi-objective optimization-based prioritization technique to assist inspections of crowdsourced test reports. In our technique, we employ the Spatial Pyramid Matching (SPM) technique to measure the similarity of the screenshots, and apply the natural-language processing technique to measure the distance between the text of test reports. Furthermore, to validate our technique, an experiment with more than 600 test reports and 2500 images is conducted. The experimental results show that image-understanding techniques can provide benefit to test-report prioritization for most applications.},
	booktitle = {2016 31st {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Feng, Yang and Jones, James A. and Chen, Zhenyu and Fang, Chunrong},
	month = sep,
	year = {2016},
	keywords = {Software, Computer bugs, Mobile applications, Testing, Crowdsourced Testing, Image color analysis, Image Understanding, Inspection, Mobile communication, Multi-Objective Optimization, Test Report Prioritization},
	pages = {202--213},
	file = {IEEE Xplore Abstract Record:files/13/7582758.html:text/html},
}

@inproceedings{reddy_spider_2015,
	title = {{SPIDER} {SENSE}: {Software}-engineering, {Networked}, {System} {Evaluation}},
	shorttitle = {{SPIDER} {SENSE}},
	doi = {10.1109/VISSOFT.2015.7332438},
	abstract = {Today, many of the research innovations in software visualization and comprehension are evaluated on small-scale programs in a way that avoids actual human evaluation, despite the fact that these techniques are designed to help programmers develop and understand large and complex software. The investments required to perform such human studies often outweigh the need to publish. As such, the goal of this work (and toolkit) is to enable the evaluation of software visualizations of real-life software systems by its actual developers, as well as to understand the factors that influence adoption. The approach is to directly assist practicing software developers with visualizations through open and online collaboration tools. The mechanism by which we accomplish this goal is an online service that is linked through the projects' revision-control and build systems. We are calling this system SPIDER SENSE, and it includes web-based visualizations for software exploration that is supported by tools for mirroring development activities, automatic building and testing, and automatic instrumentation to gather dynamic-analysis data. In the future, we envision the system and toolkit to become a framework on which further visualizations and analyses are developed. SPIDER SENSE is open-source and publicly available for download and collaborative development.},
	booktitle = {2015 {IEEE} 3rd {Working} {Conference} on {Software} {Visualization} ({VISSOFT})},
	author = {Reddy, Nishaanth H. and Kim, Junghun and Palepu, Vijay Krishna and Jones, James A.},
	month = sep,
	year = {2015},
	keywords = {Testing, Measurement, Instruments, Open source software, Software systems, Visualization},
	pages = {205--209},
	file = {IEEE Xplore Abstract Record:files/15/7332438.html:text/html},
}

@inproceedings{palepu_revealing_2015,
	title = {Revealing {Runtime} {Features} {And} {Constituent} {Behaviors} within {Software}},
	doi = {10.1109/VISSOFT.2015.7332418},
	abstract = {Software engineers organize source code into a dominant hierarchy of components and modules that may emphasize various characteristics over runtime behavior. In this way, runtime features may involve cross-cutting aspects of code from multiple components, and some of these features may be emergent in nature, rather than designed. Although source-code modularization assists software engineers to organize and find components, identifying such cross-cutting feature sets can be more difficult. This work presents a visualization that includes a static (i.e., compile-time) representation of source code that gives prominence to clusters of cooperating source-code instructions to identify dynamic (i.e., runtime) features and constituent behaviors within executions of the software. In addition, the visualization animates software executions to reveal which feature clusters are executed and in what order. The result has revealed the principal behaviors of software executions, and those behaviors were revealed to be (in some cases) cohesive, modular source-code structures and (in other cases) cross-cutting, emergent behaviors that involve multiple modules. In this paper, we describe our system (CEREBRO), envisage the uses to which it can be put, and evaluate its ability to reveal emergent runtime features and internal constituent behaviors of execution. We found that: (1) the visualization revealed emergent and commonly occuring functionalities that cross-cut the structural decomposition of the system; (2) four independent judges generally agreed in their interpretations of the code clusters, especially when informed only by our visualization; and (3) interacting with the external interface of an application while simultaneously observing the internal execution facilitated localization of code that implements the features and functionality evoked externally.},
	booktitle = {2015 {IEEE} 3rd {Working} {Conference} on {Software} {Visualization} ({VISSOFT})},
	author = {Palepu, Vijay Krishna and Jones, James A.},
	month = sep,
	year = {2015},
	keywords = {Software, Image color analysis, Visualization, Brain, Data visualization, Layout, Runtime},
	pages = {86--95},
	file = {IEEE Xplore Abstract Record:files/17/7332418.html:text/html},
}

@inproceedings{feng_test_2015,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2015},
	title = {Test report prioritization to assist crowdsourced testing},
	isbn = {978-1-4503-3675-8},
	url = {https://doi.org/10.1145/2786805.2786862},
	doi = {10.1145/2786805.2786862},
	abstract = {In crowdsourced testing, users can be incentivized to perform testing tasks and report their results, and because crowdsourced workers are often paid per task, there is a financial incentive to complete tasks quickly rather than well. These reports of the crowdsourced testing tasks are called "test reports" and are composed of simple natural language and screenshots. Back at the software-development organization, developers must manually inspect the test reports to judge their value for revealing faults. Due to the nature of crowdsourced work, the number of test reports are often difficult to comprehensively inspect and process. In order to help with this daunting task, we created the first technique of its kind, to the best of our knowledge, to prioritize test reports for manual inspection. Our technique utilizes two key strategies: (1) a diversity strategy to help developers inspect a wide variety of test reports and to avoid duplicates and wasted effort on falsely classified faulty behavior, and (2) a risk strategy to help developers identify test reports that may be more likely to be fault-revealing based on past observations. Together, these strategies form our DivRisk strategy to prioritize test reports in crowd- sourced testing. Three industrial projects have been used to evaluate the effectiveness of test report prioritization methods. The results of the empirical study show that: (1) DivRisk can significantly outperform random prioritization; (2) DivRisk can approximate the best theoretical result for a real-world industrial mobile application. In addition, we provide some practical guidelines of test report prioritization for crowdsourced testing based on the empirical study and our experiences.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 2015 10th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Feng, Yang and Chen, Zhenyu and Jones, James A. and Fang, Chunrong and Xu, Baowen},
	month = aug,
	year = {2015},
	keywords = {Crowdsourcing testing, natural language processing, test diversity, test report prioritization},
	pages = {225--236},
}

@inproceedings{palepu_discriminating_2014,
	address = {New York, NY, USA},
	series = {{ASE} '14},
	title = {Discriminating {Influences} {Among} {Instructions} in {A} {Dynamic} {Slice}},
	isbn = {978-1-4503-3013-8},
	url = {https://doi.org/10.1145/2642937.2642962},
	doi = {10.1145/2642937.2642962},
	abstract = {Dynamic slicing is an analysis that operates on program execution models (e.g., dynamic dependence graphs) to support the interpreation of program-execution traces. Given an execution event of interest (i.e., the slicing criterion), it solves for all instruction-execution events that either affect or are affected by that slicing criterion, and thereby reduces the search space to find influences within execution traces. Unfortunately, the resulting dynamic slices are still often prohibitively large for many uses. Despite this reduction search space, the dynamic slices are often still prohibitively large for many uses, and moreover, are provided without guidance of which and to what degree those influences are exerted. In this work, we present a novel approach to quantify the relevance of each instruction-execution event within a dynamic slice by its degree of relative influence on the slicing criterion. As such, we augment the dynamic slice with dynamic-relevance measures for each event in the slice, which can be used to guide and prioritize inspection of the events in the slice. We conducted an experiment that evaluates the ability of existing dynamic slicing and our approach, using dynamic relevance, to correctly identify sources of execution influence and state propagation. The results of the experiment show that inspections that were guided by traditional dynamic slicing to find the root cause for a failure reduced the search space by, on average, 61.3\%. Further, inspections guided with the assistance of the new dynamic relevance reduced the search space by 96.2\%.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 29th {ACM}/{IEEE} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Palepu, Vijay Krishna and Jones, James A.},
	month = sep,
	year = {2014},
	keywords = {dynamic slicing, dynamic dependence analysis, program analysis, program understanding, software analysis},
	pages = {37--42},
}

@article{digiuseppe_fault_2015,
	title = {Fault {Density}, {Fault} {Types}, and {Spectra}-based {Fault} {Localization}},
	volume = {20},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-014-9304-1},
	doi = {10.1007/s10664-014-9304-1},
	abstract = {This paper presents multiple empirical experiments that investigate the impact of fault quantity and fault type on statistical, coverage-based fault localization techniques and fault-localization interference. Fault-localization interference is a phenomenon revealed in earlier studies of coverage-based fault localization that causes faults to obstruct, or interfere, with other faults’ ability to be localized. Previously, it had been asserted that a fault-localization technique’s effectiveness was negatively correlated to the quantity of faults in the program. To investigate these beliefs, we conducted an experiment on six programs consisting of more than 72,000 multiple-fault versions. Our data suggests that the impact of multiple faults exerts a significant, but slight influence on fault-localization effectiveness. In addition, faults were categorized according to four existing fault-taxonomies and found no correlation between fault type and fault-localization interference. In general, even in the presence of many faults, at least one fault was found by fault localization with similar effectiveness. Additionally, our data exhibits that fault-localization interference is prevalent and exerts a meaningful influence that may cause a fault’s localizability to vary greatly. Because almost all real-world software contains multiple faults, these results affect the practical use and understanding of statistical fault-localization techniques.},
	language = {en},
	number = {4},
	urldate = {2023-09-05},
	journal = {Empirical Software Engineering},
	author = {DiGiuseppe, Nicholas and Jones, James A.},
	month = aug,
	year = {2015},
	keywords = {Debugging, Fault behavior, Fault localization},
	pages = {928--967},
}

@inproceedings{palepu_improving_2013,
	address = {Silicon Valley, CA, USA},
	series = {{ASE} '13},
	title = {Improving {Efficiency} of {Dynamic} {Analysis} with {Dynamic} {Dependence} {Summaries}},
	isbn = {978-1-4799-0215-6},
	url = {https://doi.org/10.1109/ASE.2013.6693066},
	doi = {10.1109/ASE.2013.6693066},
	abstract = {Modern applications make heavy use of third-party libraries and components, which poses new challenges for efficient dynamic analysis. To perform such analyses, transitive dependent components at all layers of the call stack must be monitored and analyzed, and as such may be prohibitively expensive for systems with large libraries and components. As an approach to address such expenses, we record, summarize, and reuse dynamic dataflows between inputs and outputs of components, based on dynamic control and data traces. These summarized dataflows are computed at a fine-grained instruction level; the result of which, we call "dynamic dependence summaries." Although static summaries have been proposed, to the best of our knowledge, this work presents the first technique for dynamic dependence summaries. The benefits to efficiency of such summarization may be afforded with losses of accuracy. As such, we evaluate the degree of accuracy loss and the degree of efficiency gain when using dynamic dependence summaries of library methods. On five large programs from the DaCapo benchmark (for which no existing whole-program dynamic dependence analyses have been shown to scale) and 21 versions of NANOXML, the summarized dependence analysis provided 90\% accuracy and a speed-up of 100\% (i.e., ×2), on average, when compared to traditional exhaustive dynamic dependence analysis.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 28th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Palepu, Vijay Krishna and Xu, Guoqing and Jones, James A.},
	month = nov,
	year = {2013},
	keywords = {dynamic slicing, program analysis, data-flow, dynamic analysis, summarization},
	pages = {59--69},
}

@inproceedings{servant_chronos_2013,
	title = {Chronos: {Visualizing} {Slices} of {Source}-code {History}},
	shorttitle = {Chronos},
	doi = {10.1109/VISSOFT.2013.6650547},
	abstract = {In this paper, we present CHRONOS-a tool that enables the querying, exploration, and discovery of historical change events to source code. Unlike traditional Revision-Control-System tools, CHRONOS allows queries across any subset of the code, down to the line-level, which can potentially be contiguous or disparate, even among multiple files. In addition, CHRONOS provides change history across all historical versions (i.e., it is not limited to a pairwise “diff”). The tool implements a zoom-able user interface as a visualization of the history of the queried code to provide both a high-level view of the changes, which supports pattern recognition and discovery, and a low-level view that supports semantic comprehension for tasks such as reverse engineering and identifying design rationale. In this paper, we describe use cases in which CHRONOS may be helpful, provide a motivating example to demonstrate the benefits brought by CHRONOS, and describe its visualization in detail.},
	booktitle = {2013 {First} {IEEE} {Working} {Conference} on {Software} {Visualization} ({VISSOFT})},
	author = {Servant, Francisco and Jones, James A.},
	month = sep,
	year = {2013},
	keywords = {History, Software systems, Visualization, Conferences, Data mining, Navigation},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:files/23/6650547.html:text/html},
}

@inproceedings{palepu_visualizing_2013,
	title = {Visualizing {Constituent} {Behaviors} within {Executions}},
	doi = {10.1109/VISSOFT.2013.6650537},
	abstract = {In this New Ideas and Emerging Results paper, we present a novel visualization, THE BRAIN, that reveals clusters of source code that co-execute to produce behavioral features of the program throughout and within executions. We created a clustered visualization of source-code that is informed by dynamic control flow of multiple executions; each cluster represents commonly interacting logic that composes software features. In addition, we render individual executions atop the clustered multiple-execution visualization as user-controlled animations to reveal characteristics of specific executions-these animations may provide exemplars for the clustered features and provide chronology for those behavioral features, or they may reveal anomalous behaviors that do not fit with the overall operational profile of most executions. Both the clustered multiple-execution view and the animated individual-execution view provide insights for the constituent behaviors within executions that compose behaviors of whole executions. Inspired by neural imaging of human brains of people who were subjected to various external stimuli, we designed and implemented THE BRAIN to reveal program activity during execution. The result has revealed the principal behaviors of execution, and those behaviors were revealed to be (in some cases) cohesive, modular source-code structures and (in other cases) cross-cutting, emergent behaviors that involve multiple modules. In this paper, we describe THE BRAIN and envisage the uses to which it can be put, and we provide two example usage scenarios to demonstrate its utility.},
	booktitle = {2013 {First} {IEEE} {Working} {Conference} on {Software} {Visualization} ({VISSOFT})},
	author = {Palepu, Vijay Krishna and Jones, James A.},
	month = sep,
	year = {2013},
	keywords = {Software, Visualization, Data visualization, Layout, Animation, Force, XML},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:files/25/6650537.html:text/html},
}

@inproceedings{servant_history_2012,
	address = {New York, NY, USA},
	series = {{FSE} '12},
	title = {History {Slicing}: {Assisting} {Code}-evolution {Tasks}},
	isbn = {978-1-4503-1614-9},
	shorttitle = {History slicing},
	url = {https://doi.org/10.1145/2393596.2393646},
	doi = {10.1145/2393596.2393646},
	abstract = {Many software-engineering tasks require developers to understand the history and evolution of source code. However, today's software-development techniques and tools are not well suited for the easy and efficient procurement of such information. In this paper, we present an approach called history slicing that can automatically identify a minimal number of code modifications, across any number of revisions, for any arbitrary segment of source code at fine granularity. We also present our implementation of history slicing, Chronos, that includes a novel visualization of the entire evolution for the code of interest. We provide two experiments: one experiment automatically computes 16,000 history slices to determine the benefit brought by various levels of automation, and another experiment that assesses the practical implications of history slicing for actual developers using the technique for actual software-maintenance tasks that involve code evolution. The experiments show that history slicing offered drastic improvements over the conventional techniques in three ways: (1) the amount of information needed to be examined and traced by developers was reduced by up to three orders of magnitude; (2) the correctness of developers attempting to solve software-maintenance tasks was more than doubled; and (3) the time to completion of these software-maintenance tasks was almost halved.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th {International} {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Servant, Francisco and Jones, James A.},
	month = nov,
	year = {2012},
	keywords = {mining software repositories, program comprehension, software evolution, software visualization},
	pages = {1--11},
}

@inproceedings{digiuseppe_concept-based_2012,
	address = {New York, NY, USA},
	series = {{FSE} '12},
	title = {Concept-based {Failure} {Clustering}},
	isbn = {978-1-4503-1614-9},
	url = {https://doi.org/10.1145/2393596.2393629},
	doi = {10.1145/2393596.2393629},
	abstract = {When attempting to determine the number and set of execution failures that are caused by particular faults, developers must perform an arduous task of investigating and diagnosing each individual failure. Researchers proposed failure-clustering techniques to automatically categorize failures, with the intention of isolating each culpable fault. The current techniques utilize dynamic control flow to characterize each failure to then cluster them. These existing techniques, however, are blind to the intent or purpose of each execution, other than what can be inferred by the control-flow profile. We hypothesize that semantically rich execution information can aid clustering effectiveness by categorizing failures according to which functionality they exhibit in the software. This paper presents a novel clustering method that utilizes latent-semantic-analysis techniques to categorize each failure by the semantic concepts that are expressed in the executed source code. We present an experiment comparing this new technique to traditional control-flow-based clustering. The results of the experiment showed that the semantic-concept clustering was more precise in the number of clusters produced than the traditional approach, without sacrificing cluster accuracy.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th {International} {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {DiGiuseppe, Nicholas and Jones, James A.},
	month = nov,
	year = {2012},
	keywords = {debugging, fault clustering, testing},
	pages = {1--4},
}

@inproceedings{digiuseppe_semantic_2012,
	address = {New York, NY, USA},
	series = {{FSE} '12},
	title = {Semantic {Fault} {Diagnosis}: {Automatic} {Natural}-language {Fault} {Descriptions}},
	isbn = {978-1-4503-1614-9},
	shorttitle = {Semantic fault diagnosis},
	url = {https://doi.org/10.1145/2393596.2393623},
	doi = {10.1145/2393596.2393623},
	abstract = {Before a fault can be fixed, it first must be understood. However, understanding why a system fails is often a difficult and time consuming process. While current automated-debugging techniques provide assistance in knowing where a fault is, developers are left unaided in understanding what a fault is, and why the system is failing. We present Semantic Fault Diagnosis (SFD), a technique that leverages lexicographic and dynamic information to automatically capture natural-language fault descriptors. SFD utilizes class names, method names, variable expressions, developer comments, and keywords from the source code to describe a fault. SFD can be used immediately after observing a failing execution and requires no input from developers or bug reports. In addition we present motivating examples and results from a SFD prototype to serve as a proof of concept.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th {International} {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {DiGiuseppe, Nicholas and Jones, James A.},
	month = nov,
	year = {2012},
	keywords = {program comprehension, testing, maintenance},
	pages = {1--4},
}

@inproceedings{servant_whosefault_2012,
	title = {{WhoseFault}: {Automatic} {Developer}-to-fault {Assignment} through {Fault} {Localization}},
	shorttitle = {{WhoseFault}},
	doi = {10.1109/ICSE.2012.6227208},
	abstract = {This paper describes a new technique, which automatically selects the most appropriate developers for fixing the fault represented by a failing test case, and provides a diagnosis of where to look for the fault. This technique works by incorporating three key components: (1) fault localization to inform locations whose execution correlate with failure, (2) history mining to inform which developers edited each line of code and when, and (3) expertise assignment to map locations to developers. To our knowledge, the technique is the first to assign developers to execution failures, without the need for textual bug reports. We implement this technique in our tool, WHOSEFAULT, and describe an experiment where we utilize a large, open-source project to determine the frequency in which our tool suggests an assignment to the actual developer who fixed the fault. Our results show that 81\% of the time, WHOSEFAULT produced the same developer that actually fixed the fault within the top three suggestions. We also show that our technique improved by a difference between 4\% and 40\% the results of a baseline technique. Finally, we explore the influence of each of the three components of our technique over its results, and compare our expertise algorithm against an existing expertise assessment technique and find that our algorithm provides greater accuracy, by up to 37\%.},
	booktitle = {2012 34th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Servant, Francisco and Jones, James A.},
	month = jun,
	year = {2012},
	note = {ISSN: 1558-1225},
	keywords = {Software, History, Measurement, Data mining, mining software repositories, Correlation, developer assignment, expertise assignment, fault localization, Informatics, Software algorithms},
	pages = {36--46},
	file = {IEEE Xplore Abstract Record:files/30/6227208.html:text/html},
}

@inproceedings{deng_weighted_2012,
	title = {Weighted {System} {Dependence} {Graph}},
	doi = {10.1109/ICST.2012.118},
	abstract = {In this paper, we present a weighted, hybrid program-dependence model that represents the relevance of highly related, dependent code to assist developer comprehension of the program for multiple software-engineering tasks. Programmers often need to understand the dependencies among program elements, which may exist across multiple modules. Although such dependencies can be gathered from traditional models, such as slices, the scalability of these approaches is often prohibitive for direct, practical use. To address this scalability issue, as well as to assist developer comprehension, we introduce a program model that includes static dependencies as well as information about any number of executions, which inform the weight and relevance of the dependencies. Additionally, classes of executions can be differentiated in such a way as to support multiple software-engineering tasks. We evaluate this weighted, hybrid model for a task that involves exploring the structural context while debugging. The results demonstrate that the new model more effectively reveals relevant failure-correlated code than the static-only model, thus enabling a more scalable exploration or post hoc analysis.},
	booktitle = {Verification and {Validation} 2012 {IEEE} {Fifth} {International} {Conference} on {Software} {Testing}},
	author = {Deng, Fang and Jones, James A.},
	month = apr,
	year = {2012},
	note = {ISSN: 2159-4848},
	keywords = {Analytical models, Instruments, Runtime, Debugging, dynamic analysis, debugging, Correlation, Context, Data models, fault-localization, hybrid analysis, program models, static analysis},
	pages = {380--389},
	file = {IEEE Xplore Abstract Record:files/32/6200130.html:text/html},
}

@inproceedings{digiuseppe_software_2012,
	title = {Software {Behavior} and {Failure} {Clustering}: {An} {Empirical} {Study} of {Fault} {Causality}},
	shorttitle = {Software {Behavior} and {Failure} {Clustering}},
	doi = {10.1109/ICST.2012.99},
	abstract = {To cluster executions that exhibit faulty behavior by the faults that cause them, researchers have proposed using internal execution events, such as statement profiles, to (1) measure execution similarities, (2) categorize executions based on those similarity results, and (3) suggest the resulting categories as sets of executions exhibiting uniform fault behavior. However, due to a paucity of evidence correlating profiles and output behavior, researchers employ multiple simplifying assumptions in order to justify such approaches. In this paper we present an empirical study of profile correlation with output behavior, and we reexamine the suitability of such simplifying assumptions. We examine over 4 billion test-case outputs and execution profiles from multiple programs with over 9000 versions. Our data provides evidence that with current techniques many executions should be omitted from the clustering analysis to provide clusters that each represent a single fault. In addition, our data reveals the previously undocumented effects of multiple faults on failures, which has implications for techniques' ability (and inability) to properly cluster. Our results suggest directions for the improvement of future failure-clustering techniques that better account for software-fault behavior.},
	booktitle = {Verification and {Validation} 2012 {IEEE} {Fifth} {International} {Conference} on {Software} {Testing}},
	author = {DiGiuseppe, Nicholas and Jones, James A.},
	month = apr,
	year = {2012},
	note = {ISSN: 2159-4848},
	keywords = {Software, Measurement, Debugging, Informatics, Clustering algorithms, Complexity theory, Execution Clustering, Failure Proximity, Fault Understanding, Semantics},
	pages = {191--200},
	file = {IEEE Xplore Abstract Record:files/34/6200116.html:text/html},
}

@inproceedings{martie_trendy_2012,
	title = {Trendy {Bugs}: {Topic} {Trends} in the {Android} {Bug} {Reports}},
	shorttitle = {Trendy bugs},
	doi = {10.1109/MSR.2012.6224268},
	abstract = {Studying vast volumes of bug and issue discussions can give an understanding of what the community has been most concerned about, however the magnitude of documents can overload the analyst. We present an approach to analyze the development of the Android open source project by observing trends in the bug discussions in the Android open source project public issue tracker. This informs us of the features or parts of the project that are more problematic at any given point of time. In turn, this can be used to aid resource allocation (such as time and man power) to parts or features. We support these ideas by presenting the results of issue topic distributions over time using statistical analysis of the bug descriptions and comments for the Android open source project. Furthermore, we show relationships between those time distributions and major development releases of the Android OS.},
	booktitle = {2012 9th {IEEE} {Working} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Martie, Lee and Palepu, Vijay Krishna and Sajnani, Hitesh and Lopes, Cristina},
	month = jun,
	year = {2012},
	note = {ISSN: 2160-1860},
	keywords = {Runtime, Android, Androids, bug logs, Google, Graphics, Humanoid robots, Java, Smart phones, statistical trend analysis, topics},
	pages = {120--123},
	file = {IEEE Xplore Abstract Record:files/36/6224268.html:text/html},
}

@inproceedings{clark_localizing_2011,
	title = {Localizing {SQL} {Faults} in {Database} {Applications}},
	doi = {10.1109/ASE.2011.6100056},
	abstract = {This paper presents a new fault-localization technique designed for applications that interact with a relational database. The technique uses dynamic information specific to the application's database, such as Structured Query Language (SQL) commands, to provide a fault-location diagnosis. By creating statement-SQL tuples and calculating their suspiciousness, the presented method lets the developer identify the database commands and the program statements likely to cause the failures. The technique also calculates suspiciousness for statement-attribute tuples and uses this information to identify SQL fragments that are statistically likely to be responsible for the suspiciousness of that SQL command. The paper reports the results of two empirical studies. The first study compares existing and database-aware fault-localization methods, and reveals the strengths and limitations of prior techniques, while also highlighting the effectiveness of the new approach. The second study demonstrates the benefits of using database information to improve understanding and reduce manual debugging effort.},
	booktitle = {2011 26th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE} 2011)},
	author = {Clark, Sarah R. and Cobb, Jake and Kapfhammer, Gregory M. and Jones, James A. and Harrold, Mary Jean},
	month = nov,
	year = {2011},
	note = {ISSN: 1938-4300},
	keywords = {Software, Measurement, Instruments, Java, Marketing and sales, Relational databases},
	pages = {213--222},
	file = {IEEE Xplore Abstract Record:files/39/6100056.html:text/html;Submitted Version:files/38/Clark et al. - 2011 - Localizing SQL faults in database applications.pdf:application/pdf},
}

@inproceedings{deng_inferred_2011,
	title = {Inferred {Dependence} {Coverage} to {Support} {Fault} {Contextualization}},
	doi = {10.1109/ASE.2011.6100112},
	abstract = {This paper provides techniques for aiding developers' task of familiarizing themselves with the context of a fault. Many fault-localization techniques present the software developer with a subset of the program to inspect in order to aid in the search for faults that cause failures. However, typically, these techniques do not describe how the components of the subset relate to each other in a way that enables the developer to understand how these components interact to cause failures. These techniques also do not describe how the subset relates to the rest of the program in a way that enables the developer to understand the context of the subset. In this paper, we present techniques for providing static and dynamic relations among program elements that can be used as the basis for the exploration of a program when attempting to understand the nature of faults.},
	booktitle = {2011 26th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE} 2011)},
	author = {Deng, Fang and Jones, James A.},
	month = nov,
	year = {2011},
	note = {ISSN: 1938-4300},
	keywords = {Software, Instruments, Conferences, Context, Context modeling, Software engineering, USA Councils},
	pages = {512--515},
	file = {IEEE Xplore Abstract Record:files/41/6100112.html:text/html},
}

@inproceedings{servant_history_2011,
	title = {History {Slicing}},
	doi = {10.1109/ASE.2011.6100097},
	abstract = {To perform a number of tasks such as inferring design rationale from past code changes or assessing developer expertise for a software feature or bug, the evolution of a set of lines of code can be assessed by mining software histories. However, determining the evolution of a set of lines of code is a manual and time consuming process. This paper presents a model of this process and an approach for automating it. We call this process History Slicing. We describe the process and options for generating a graph that links every line of code with its corresponding previous revision through the history of the software project. We then explain the method and options for utilizing this graph to determine the exact revisions that contain changes for the lines of interest and their exact position in each revision. Finally, we present some preliminary results which show initial evidence that our automated technique can be several orders of magnitude faster than the manual approach and require that developers examine up to two orders of magnitude less code in extracting such histories.},
	booktitle = {2011 26th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE} 2011)},
	author = {Servant, Francisco and Jones, James A.},
	month = nov,
	year = {2011},
	note = {ISSN: 1938-4300},
	keywords = {Software, History, Conferences, Data mining, Software engineering, USA Councils, Manuals},
	pages = {452--455},
	file = {IEEE Xplore Abstract Record:files/43/6100097.html:text/html},
}

@inproceedings{deng_constellation_2011,
	title = {Constellation {Visualization}: {Augmenting} {Program} {Dependence} with {Dynamic} {Information}},
	shorttitle = {Constellation visualization},
	doi = {10.1109/VISSOF.2011.6069453},
	abstract = {This paper presents a scalable, statement-level visualization that shows related code in a way that supports human interpretation of clustering and context. The visualization is applicable to many software-engineering tasks through the utilization and visualization of problem-specific meta-data. The visualization models statement-level code relations from a system-dependence-graph model of the program being visualized. Dynamic, run-time information is used to augment the static program model to further enable visual cluster identification and interpretation. In addition, we performed a user study of our visualization on an example program domain. The results of the study show that our new visualization successfully revealed relevant context to the programmer participants.},
	booktitle = {2011 6th {International} {Workshop} on {Visualizing} {Software} for {Understanding} and {Analysis} ({VISSOFT})},
	author = {Deng, Fang and DiGiuseppe, Nicholas and Jones, James A.},
	month = sep,
	year = {2011},
	keywords = {Software, Visualization, Layout, Context, Color, Scalability, Springs},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:files/45/6069453.html:text/html;Submitted Version:files/46/Deng et al. - 2011 - Constellation visualization Augmenting program de.pdf:application/pdf},
}

@inproceedings{digiuseppe_fault_2011,
	title = {Fault {Interaction} and {Its} {Repercussions}},
	doi = {10.1109/ICSM.2011.6080767},
	abstract = {Multiple faults in a program can interact to form new behaviors in a program that would not be realized if the program were to contain the individual faults. This paper presents an in-depth study of the effects of the interaction of faults within a program. Many researchers attempt to ameliorate the effects of faulty programs. Unfortunately, such researchers are left to rely upon intuition about fault behavior due to the paucity of formalized studies of faults and their behavior. In an attempt to advance the understanding of faults and their behavior, we conducted a study of fault interaction across six subjects with more than 65,000 multiple-fault versions. The results of our study show four significant types of interaction, with one type - faults obscuring the effects of other faults - as the most prevalent type. The prevalence of obscuring faults' effects has an adverse effect on many automated software-engineering techniques, such as regression-testing, fault-localization, and fault-clustering techniques. Given that software commonly contains more than a single fault, these results have implications for developers and researchers alike by informing them of expected complications, which in many instances are opposite to intuition.},
	booktitle = {2011 27th {IEEE} {International} {Conference} on {Software} {Maintenance} ({ICSM})},
	author = {DiGiuseppe, Nicholas and Jones, James A.},
	month = sep,
	year = {2011},
	note = {ISSN: 1063-6773},
	keywords = {Software, Testing, Informatics, Flexible printed circuits, Interference, Noise, Schedules},
	pages = {3--12},
	file = {IEEE Xplore Abstract Record:files/48/6080767.html:text/html},
}

@inproceedings{cobb_dynamic_2011,
	address = {New York, NY, USA},
	series = {{WODA} '11},
	title = {Dynamic {Invariant} {Detection} for {Relational} {Databases}},
	isbn = {978-1-4503-0811-3},
	url = {https://doi.org/10.1145/2002951.2002955},
	doi = {10.1145/2002951.2002955},
	abstract = {Despite the many automated techniques that benefit from dynamic invariant detection, to date, none are able to capture and detect dynamic invariants at the interface of a program and its databases. This paper presents a dynamic invariant detection method for relational databases and for programs that use relational databases and an implementation of the approach that leverages the Daikon dynamic-invariant engine. The method defines a mapping between relational database elements and Daikon's notion of program points and variable observations, thus enabling row-level and column-level invariant detection. The paper also presents the results of two empirical evaluations on four fixed data sets and three subject programs. The first study shows that dynamically detecting and inferring invariants in a relational database is feasible and 55\% of the invariants produced for each subject are meaningful. The second study reveals that all of these meaningful invariants are schema-enforceable using standards-compliant databases and many can be checked by databases with only limited schema constructs.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the {Ninth} {International} {Workshop} on {Dynamic} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Cobb, Jake and Jones, James A. and Kapfhammer, Gregory M. and Harrold, Mary Jean},
	month = jul,
	year = {2011},
	keywords = {dynamic invariants, relational databases, schema modification},
	pages = {12--17},
}

@inproceedings{digiuseppe_influence_2011,
	address = {New York, NY, USA},
	series = {{ISSTA} '11},
	title = {On {The} {Influence} of {Multiple} {Faults} on {Coverage}-based {Fault} {Localization}},
	isbn = {978-1-4503-0562-4},
	url = {https://doi.org/10.1145/2001420.2001446},
	doi = {10.1145/2001420.2001446},
	abstract = {This paper presents an empirical study on the effects of the quantity of faults on statistical, coverage-based fault localization techniques. The former belief was that the effectiveness of fault-localization techniques was inversely proportional to the quantity of faults. In an attempt to verify these beliefs, we conducted a study on three programs varying in size on more than 13,000 multiple-fault versions. We found that the influence of multiple faults (1) was not as great as expected, (2) created a negligible effect on the effectiveness of the fault localization, and (3) was often even complimentary to the fault-localization effectiveness. In general, even in the presence of many faults, at least one fault was found by the fault-localization technique with high effectiveness. We also found that some faults were localizable regardless of the presence of other faults, whereas other faults' ability to be found by these techniques varied greatly in the presence of other faults. Because almost all real-world software contains multiple faults, these results impact the use of statistical fault-localization techniques and provide a greater understanding of their potential in practice.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 2011 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {DiGiuseppe, Nicholas and Jones, James A.},
	month = jul,
	year = {2011},
	keywords = {debugging, fault localization, empirical studies},
	pages = {210--220},
}

@inproceedings{grechanik_bridging_2010,
	address = {New York, NY, USA},
	series = {{FoSER} '10},
	title = {Bridging {Gaps} between {Developers} and {Testers} in {Globally}-distributed {Software} {Development}},
	isbn = {978-1-4503-0427-6},
	url = {https://doi.org/10.1145/1882362.1882394},
	doi = {10.1145/1882362.1882394},
	abstract = {One of the main challenges in distributed development is ensuring effective communication and coordination among the distributed teams. In this context, little attention has been paid so far to coordination in software testing. In distributed development environments, testing is often performed by specialized teams that operate as independent quality assurance centers. The use of these centers can be advantageous from both an economic and a software quality perspective. These benefits, however, are offset by severe difficulties in coordination between testing and software development centers. Test centers operate as isolated silos and have little to no interactions with developers, which can result in multiple problems that lead to poor quality of software. Based on our preliminary investigation, we claim that we need to rethink the way testing is performed in distributed development environments. We then present a possible research agenda that would help address the identified issues and discuss the main challenges involved.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the {FSE}/{SDP} workshop on {Future} of software engineering research},
	publisher = {Association for Computing Machinery},
	author = {Grechanik, Mark and Jones, James A. and Orso, Alessandro and van der Hoek, André},
	month = nov,
	year = {2010},
	keywords = {distributed software development, global software engineering, software test centers},
	pages = {149--154},
}

@inproceedings{servant_casi_2010,
	address = {New York, NY, USA},
	series = {{CHASE} '10},
	title = {{CASI}: {Preventing} {Indirect} {Conflicts} through {A} {Live} {Visualization}},
	isbn = {978-1-60558-966-4},
	shorttitle = {{CASI}},
	url = {https://doi.org/10.1145/1833310.1833317},
	doi = {10.1145/1833310.1833317},
	abstract = {Software development is a collaborative activity that may lead to conflicts when changes are performed in parallel by several developers. Direct conflicts arise when multiple developers make changes in the same source code entity, and indirect conflicts are produced when multiple developers make changes to source code entities that depend on each other. Previous approaches of code analysis either cannot predict all kinds of indirect conflicts, since they can be caused by syntactic or semantic changes, or they produce so much information as to make them virtually useless. Workspace awareness techniques have been proposed to enhance software configuration management systems by providing developers with information about the activity that is being performed by other developers. Most workspace awareness tools detect direct conflicts while only some of them warn about potential indirect conflicts. We propose a new approach to the problem of indirect conflicts. Our tool CASI informs developers of the changes that are taking place in a software project and the source code entities influenced by them. We visualize this influence together with directionality and severity information to help developers decide whether a concrete situation represents an indirect conflict. We introduce our approach, explain its implementation, discuss its behavior on an example, and lay out several steps that we will be taking to improve it in the future.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 2010 {ICSE} {Workshop} on {Cooperative} and {Human} {Aspects} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Servant, Francisco and Jones, James A. and van der Hoek, André},
	month = may,
	year = {2010},
	keywords = {software visualization, conflicts, parallel work, software configuration management, workspace awareness},
	pages = {39--46},
}

@article{wang_online_2009,
	title = {An {Online} {Monitoring} {Approach} for {Web} {Service} {Requirements}},
	volume = {2},
	issn = {1939-1374},
	doi = {10.1109/TSC.2009.22},
	abstract = {Web service technology aims to enable the interoperation of heterogeneous systems and the reuse of distributed functions in an unprecedented scale and has achieved significant success. There are still, however, challenges to realize its full potential. One of these challenges is to ensure the behavior of Web services consistent with their requirements. Monitoring events that are relevant to Web service requirements is, thus, an important technique. This paper introduces an online monitoring approach for Web service requirements. It includes a pattern-based specification of service constraints that correspond to service requirements, and a monitoring model that covers five kinds of system events relevant to client request, service response, application, resource, and management, and a monitoring framework in which different probes and agents collect events and data that are sensitive to requirements. The framework analyzes the collected information against the prespecified constraints, so as to evaluate the behavior and use of Web services. The prototype implementation and experiments with a case study shows that our approach is effective and flexible, and the monitoring cost is affordable.},
	number = {4},
	journal = {IEEE Transactions on Services Computing},
	author = {Wang, Qianxiang and Shao, Jin and Deng, Fang and Liu, Yonggang and Li, Min and Han, Jun and Mei, Hong},
	month = oct,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Services Computing},
	keywords = {Software, Data mining, XML, Context, constraints., monitoring, Monitoring, requirements, Security, Web services},
	pages = {338--351},
	file = {IEEE Xplore Abstract Record:files/54/5196661.html:text/html},
}

@inproceedings{deng_toward_2009,
	title = {Toward {Middleware}-{Based} {Online} {Application} {Migration}},
	volume = {1},
	doi = {10.1109/COMPSAC.2009.36},
	abstract = {As an effective approach to maintain software system without interrupting the service, online migration has been applied for many goals, e.g. achieving a higher system performance, tolerating faults, and upgrading systems. In this paper, we propose a middleware-based approach for online application migration. Independent from the operating system, middleware not only provides information required to perform migration, but also offers a flexible and efficient way for online application migration. We implement our approach on the J2EE platform, one of the most important middleware platforms in recent years. A J2EE ap plication migration framework (JAMF) is developed to demonstrate the proposed approach. The experiments with two typical examples show that our approach is feasible and effective.},
	booktitle = {2009 33rd {Annual} {IEEE} {International} {Computer} {Software} and {Applications} {Conference}},
	author = {Deng, Fang and Wang, Qianxiang and Shao, Jin},
	month = jul,
	year = {2009},
	note = {ISSN: 0730-3157},
	keywords = {Software systems, application, application migration, Application software, Bandwidth, component, Computer applications, middleware, Middleware, migration, online migration, Operating systems, Software maintenance, Software performance, System performance, Virtual machining},
	pages = {222--227},
	file = {IEEE Xplore Abstract Record:files/56/5254259.html:text/html},
}

@inproceedings{jones_enabling_2009,
	title = {Enabling and {Enhancing} {Collaborations} between {Software} {Development} {Organizations} and {Independent} {Test} {Agencies}},
	doi = {10.1109/CHASE.2009.5071411},
	abstract = {While the use of independent test agencies is on the rise - currently estimated to be a \$25B marketplace - there are a number of challenges to successful collaboration between these agencies and their client software development organizations. These agencies offer independent verification of software, skilled testing experts, and economic advantages that arise from differential global labor rates. However, these benefits are often offset by difficulties in effectively integrating the outsourced testing into software development practice. We conducted extensive discussions with test managers and engineers at software development organizations. This position paper presents the findings of these discussions that identify key difficulties of integrating independent test agencies into software development practice, and it describes our position on how these findings can be addressed.},
	booktitle = {2009 {ICSE} {Workshop} on {Cooperative} and {Human} {Aspects} on {Software} {Engineering}},
	author = {Jones, James A. and Grechanik, Mark and van der Hoek, Andre},
	month = may,
	year = {2009},
	keywords = {Informatics, Collaborative software, Costs, Economics, Engineering management, Outsourcing, Programming, Software development management, Software testing, System testing},
	pages = {56--59},
	file = {IEEE Xplore Abstract Record:files/58/similar.html:text/html},
}

@inproceedings{santelices_lightweight_2009,
	title = {Lightweight {Fault}-localization {Using} {Multiple} {Coverage} {Types}},
	doi = {10.1109/ICSE.2009.5070508},
	abstract = {Lightweight fault-localization techniques use program coverage to isolate the parts of the code that are most suspicious of being faulty. In this paper, we present the results of a study of three types of program coverage—statements, branches, and data dependencies—to compare their effectiveness in localizing faults. The study shows that no single coverage type performs best for all faults—different kinds of faults are best localized by different coverage types. Based on these results, we present a new coverage-based approach to fault localization that leverages the unique qualities of each coverage type by combining them. Because data dependencies are noticeably more expensive to monitor than branches, we also investigate the effects of replacing data-dependence coverage with an approximation inferred from branch coverage. Our empirical results show that (1) the cost of fault localization using combinations of coverage is less than using any individual coverage type and closer to the best case (without knowing in advance which kinds of faults are present), and (2) using inferred data-dependence coverage retains most of the benefits of combinations.},
	booktitle = {2009 {IEEE} 31st {International} {Conference} on {Software} {Engineering}},
	author = {Santelices, Raul and Jones, James A. and Yu, Yanbing and Harrold, Mary Jean},
	month = may,
	year = {2009},
	note = {ISSN: 1558-1225},
	keywords = {Instruments, Runtime, Informatics, Java, Costs, Condition monitoring, Educational institutions, Isolation technology, Performance evaluation, Software debugging},
	pages = {56--66},
	file = {IEEE Xplore Abstract Record:files/60/5070508.html:text/html},
}

@inproceedings{hsu_rapid_2008,
	title = {Rapid: {Identifying} {Bug} {Signatures} to {Support} {Debugging} {Activities}},
	shorttitle = {Rapid},
	doi = {10.1109/ASE.2008.68},
	abstract = {Most existing fault-localization techniques focus on identifying and reporting single statements that may contain a fault. Even in cases where a fault involves a single statement, it is generally hard to understand the fault by looking at that statement in isolation. Faults typically manifest themselves in a specific context, and knowing that context is necessary to diagnose and correct the fault. In this paper, we present a novel fault-localization technique that identifies sequences of statements that lead to a failure. The technique works by analyzing partial execution traces corresponding to failing executions and identifying common segments in these traces, incrementally. Our approach provides developers a context that is likely to result in a more directed approach to fault understanding and a lower overall cost for debugging.},
	booktitle = {2008 23rd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	author = {Hsu, Hwa-You and Jones, James A. and Orso, Alessandro},
	month = sep,
	year = {2008},
	note = {ISSN: 1938-4300},
	keywords = {Instruments, Debugging, Data mining, Software engineering, USA Councils, Fault diagnosis, Information filters},
	pages = {439--442},
	file = {IEEE Xplore Abstract Record:files/62/4639361.html:text/html},
}

@inproceedings{yu_empirical_2008,
	title = {An {Empirical} {Study} of {The} {Effects} of {Test}-suite {Reduction} on {Fault} {Localization}},
	doi = {10.1145/1368088.1368116},
	abstract = {Fault-localization techniques that utilize information about all test cases in a test suite have been presented. These techniques use various approaches to identify the likely faulty part(s) of a program, based on information about the execution of the program with the test suite. Researchers have begun to investigate the impact that the composition of the test suite has on the effectiveness of these fault-localization techniques. In this paper, we present the first experiment on one aspect of test-suite composition–test-suite reduction. Our experiment studies the impact of the test-suite reduction on the effectiveness of fault-localization techniques. In our experiment, we apply 10 test-suite reduction strategies to test suites for eight subject programs. We then measure the differences between the effectiveness of four existing fault-localization techniques on the unreduced and reduced test suites. We also measure the reduction in test-suite size of the 10 test-suite reduction strategies. Our experiment shows that fault-localization effectiveness varies depending on the test-suite reduction strategy used, and it demonstrates the trade-offs between test-suite reduction and fault-localization effectiveness.},
	booktitle = {2008 {ACM}/{IEEE} 30th {International} {Conference} on {Software} {Engineering}},
	author = {Yu, Yanbing and Jones, James and Harrold, Mary Jean},
	month = may,
	year = {2008},
	note = {ISSN: 1558-1225},
	keywords = {fault localization, Software engineering, Software testing, Educational institutions, Software debugging, Fault diagnosis, empirical study, Permission, Size measurement, test-suite reduction},
	pages = {201--210},
	file = {IEEE Xplore Abstract Record:files/64/4814131.html:text/html},
}

@inproceedings{alrahem_interstate_2007,
	title = {{INTERSTATE}: {A} {Stateful} {Protocol} {Fuzzer} for {SIP}},
	shorttitle = {{INTERSTATE}},
	url = {https://www.semanticscholar.org/paper/INTERSTATE%3A-A-Stateful-Protocol-Fuzzer-for-SIP-Alrahem-Chen/ced3d57d6f67bc8d14bbd252b43c5d74aab5fc94},
	abstract = {We present the INTERSTATE fuzzer to detect security vulnerabilities in VOIP phones which implement Session Initiation Protocol (SIP). INTERSTATE generates an input sequence for a SIP phone which is constructed to reveal common security vulnerabilities. SIP is a stateful protocol so a state machine description of the SIP protocol is used by INTERSTATE to ensure that the entire state space is explored. The input sequence consists of SIP request messages as well as GUI input sequences which are remotely applied to the phone under test. The input sequence is generated to perform a random walk through the state space of the protocol. The application of GUI inputs is essential to ensure that all parts of the state machine can be tested. Faults are injected into SIP messages to trigger common vulnerabilities. INTERSTATE also checks the SIP response messages received from the phone under test against the expected responses described in the state machine. Checking response messages allows for the detection of security bugs whose impact is more subtle than a simple crash. We have used INTERSTATE to identify a previously unknown DoS vulnerability in an existing open source SIP phone. The vulnerability could not have been discovered without exploring multiple paths through the state machine, and applying GUI inputs during the fuzzing process.},
	urldate = {2023-09-05},
	author = {Alrahem, Thoulfekar and Chen, Alex and DiGiussepe, Nick and Gee, J. and Hsiao, Shang-Pin and Mattox, Sean and Park, Taejoon and Tam, Albert and Harris, Ian G.},
	year = {2007},
	annote = {[TLDR] The INTERSTATE fuzzer is presented to detect security vulnerabilities in VOIP phones which implement Session Initiation Protocol (SIP), and has been used to identify a previously unknown DoS vulnerability in an existing open source SIP phone.},
}

@inproceedings{jones_debugging_2007,
	address = {New York, NY, USA},
	series = {{ISSTA} '07},
	title = {Debugging in {Parallel}},
	isbn = {978-1-59593-734-6},
	url = {https://doi.org/10.1145/1273463.1273468},
	doi = {10.1145/1273463.1273468},
	abstract = {The presence of multiple faults in a program can inhibit the ability of fault-localization techniques to locate the faults. This problem occurs for two reasons: when a program fails, the number of faults is, in general, unknown; and certain faults may mask or obfuscate other faults. This paper presents our approach to solving this problem that leverages the well-known advantages of parallel work flows to reduce the time-to-release of a program. Our approach consists of a technique that enables more effective debugging in the presence of multiple faults and a methodology that enables multiple developers to simultaneously debug multiple faults. The paper also presents an empirical study that demonstrates that our parallel-debugging technique and methodology can yield a dramatic decrease in total debugging time compared to a one-fault-at-a-time, or conventionally sequential, approach.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 2007 international symposium on {Software} testing and analysis},
	publisher = {Association for Computing Machinery},
	author = {Jones, James A. and Bowring, James F. and Harrold, Mary Jean},
	month = jul,
	year = {2007},
	keywords = {program analysis, fault localization, empirical study, automated debugging, execution clustering},
	pages = {16--26},
}

@inproceedings{jones_empirical_2005,
	address = {New York, NY, USA},
	series = {{ASE} '05},
	title = {Empirical {Evaluation} of {The} {Tarantula} {Automatic} {Fault}-localization {Technique}},
	isbn = {978-1-58113-993-8},
	url = {https://doi.org/10.1145/1101908.1101949},
	doi = {10.1145/1101908.1101949},
	abstract = {The high cost of locating faults in programs has motivated the development of techniques that assist in fault localization by automating part of the process of searching for faults. Empirical studies that compare these techniques have reported the relative effectiveness of four existing techniques on a set of subjects. These studies compare the rankings that the techniques compute for statements in the subject programs and the effectiveness of these rankings in locating the faults. However, it is unknown how these four techniques compare with Tarantula, another existing fault-localization technique, although this technique also provides a way to rank statements in terms of their suspiciousness. Thus, we performed a study to compare the Tarantula technique with the four techniques previously compared. This paper presents our study---it overviews the Tarantula technique along with the four other techniques studied, describes our experiment, and reports and discusses the results. Our studies show that, on the same set of subjects, the Tarantula technique consistently outperforms the other four techniques in terms of effectiveness in fault localization, and is comparable in efficiency to the least expensive of the other four techniques.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 20th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Jones, James A. and Harrold, Mary Jean},
	month = nov,
	year = {2005},
	keywords = {program analysis, fault localization, empirical study, automated debugging},
	pages = {273--282},
}

@inproceedings{orso_gamma_2004,
	title = {{GAMMA}℡{LA}: {Visualization} of {Program}-{Execution} {Data} for {Deployed} {Software}},
	shorttitle = {{GAMMA}℡{LA}},
	doi = {10.1109/ICSE.2004.1317495},
	abstract = {To investigate the program-execution data efficiently, we must be able to view the data at different levels of detail. In our visualization approach, we represent software systems at three different levels: statement level, file level, and system level. At the statement level, we represent the actual code. The representation at the file level provides a miniaturized view of the source code similar to the one used in the SeeSoft system (Eick et al., 1992). The system level uses treemaps (Shneiderman, 1992 and Bruls et al., 2000) to represent the software and is the most abstracted level in our visualization. At each level, coloring is used to represent one- or two-dimensional information about the code, using the colors' hue and brightness components. The coloring technique that we apply is a generalization of the coloring technique defined for fault-localization by Jones and colleagues (2001). GAMMA℡LA is a toolset that implements our visualization approach and provides capabilities for instrumenting the code, collecting program-execution data from the field, and storing and retrieving the data locally. GAMMA℡LA is written in Java, supports the monitoring of Java programs, and consists of three main components: an instrumentation, execution, and coverage tool, a data collection daemon, and a program visualizer.},
	booktitle = {Proceedings. 26th {International} {Conference} on {Software} {Engineering}},
	author = {Orso, A. and Jones, James A. and Harrold, Mary Jean and Stasko, John},
	month = may,
	year = {2004},
	note = {ISSN: 0270-5257},
	keywords = {Testing, Instruments, Data visualization, Monitoring, Educational institutions, Data analysis, Optimization, Performance analysis, Quality assurance, Software safety},
	pages = {699--700},
	file = {IEEE Xplore Abstract Record:files/71/1317495.html:text/html},
}

@inproceedings{jones_fault_2004,
	title = {Fault {Localization} {Using} {Visualization} of {Test} {Information}},
	doi = {10.1109/ICSE.2004.1317420},
	abstract = {Attempts to reduce the number of delivered faults in software are estimated to consume 50\% to 80\% of the development and maintenance effort according to J.S. Collofello ans S.N. Woodfield (1989). Among the tasks required to reduce the number of delivered faults, debugging is one of the most time-consuming according to T. Ball and S.G. Eick and Telcordia Technologies, and locating the errors is the most difficult component of this debugging task according to I. Vessey (1985). Clearly, techniques that can reduce the time required to locate faults can have a significant impact on the cost and quality of software development and maintenance.},
	booktitle = {Proceedings. 26th {International} {Conference} on {Software} {Engineering}},
	author = {Jones, James A.},
	month = may,
	year = {2004},
	note = {ISSN: 0270-5257},
	keywords = {Visualization, Debugging, Software maintenance, Costs, Programming, Software testing, Fault diagnosis, Displays, Failure analysis, Information analysis},
	pages = {54--56},
	file = {IEEE Xplore Abstract Record:files/73/1317420.html:text/html},
}

@inproceedings{orso_visualization_2003,
	address = {New York, NY, USA},
	series = {{SoftVis} '03},
	title = {Visualization of {Program}-execution {Data} for {Deployed} {Software}},
	isbn = {978-1-58113-642-5},
	url = {https://doi.org/10.1145/774833.774843},
	doi = {10.1145/774833.774843},
	abstract = {Software products are often released with missing functionality, errors, or incompatibilities that may result in failures in the field, inferior performances, or, more generally, user dissatisfaction. In previous work, we presented the GAMMA technology, which facilitates remote analysis and measurement of deployed software and allows for gathering programexecution data from the field. When monitoring a high number of deployed instances of a software product, however, a large amount of data is collected. Such raw data are useless in the absence of a suitable data-mining and visualization technique that supports exploration and understanding of the data. In this paper, we present a new technique for collecting, storing, and visualizing program-execution data gathered from deployed instances of a software product. We also present a prototype toolset, GAMMATELLA, that implements the technique. We show how the visualization capabilities of GAMMATELLA allows for effectively investigating several kinds of execution-related information in an interactive fashion.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 2003 {ACM} symposium on {Software} visualization},
	publisher = {Association for Computing Machinery},
	author = {Orso, Alessandro and Jones, James and Harrold, Mary Jean},
	month = jun,
	year = {2003},
	keywords = {software visualization, Gamma technology, remote monitoring},
	pages = {67--ff},
	file = {Full Text:files/75/Orso et al. - 2003 - Visualization of program-execution data for deploy.pdf:application/pdf},
}

@inproceedings{jones_test-suite_2001,
	title = {Test-suite {Reduction} and {Prioritization} for {Modified} {Condition}/{Decision} {Coverage}},
	doi = {10.1109/ICSM.2001.972715},
	abstract = {Software testing is particularly expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One reason for this expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, there is evidence that MC/DC is an effective verification technique, and can help to uncover safety faults. As the software is modified and new test cases are added to the test suite, the test suite grows, and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software, according to some criterion, as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite according to some criteria or goals. Existing test-suite reduction and prioritization techniques, however, may not be effective in reducing or prioritizing MC/DC-adequate test suites because they do not consider the complexity of the criterion. The paper presents new algorithms for test-suite reduction and prioritization that can be tailored effectively for use with MC/DC. The paper also presents the results of a case study of the test-suite reduction algorithm.},
	booktitle = {Proceedings {IEEE} {International} {Conference} on {Software} {Maintenance}. {ICSM} 2001},
	author = {Jones, James A. and Harrold, Mary Jean},
	month = nov,
	year = {2001},
	note = {ISSN: 1063-6773},
	keywords = {Software algorithms, Software maintenance, Costs, Software testing, System testing, Educational institutions, Performance evaluation, Electronic switching systems, FAA, Safety},
	pages = {92--101},
	file = {IEEE Xplore Abstract Record:files/77/972715.html:text/html},
}

@inproceedings{jones_visualization_2002,
	address = {New York, NY, USA},
	series = {{ICSE} '02},
	title = {Visualization of {Test} {Information} to {Assist} {Fault} {Localization}},
	isbn = {978-1-58113-472-8},
	url = {https://doi.org/10.1145/581339.581397},
	doi = {10.1145/581339.581397},
	abstract = {One of the most expensive and time-consuming components of the debugging process is locating the errors or faults. To locate faults, developers must identify statements involved in failures and select suspicious statements that might contain faults. This paper presents a new technique that uses visualization to assist with these tasks. The technique uses color to visually map the participation of each program statement in the outcome of the execution of the program with a test suite, consisting of both passed and failed test cases. Based on this visual mapping, a user can inspect the statements in the program, identify statements involved in failures, and locate potentially faulty statements. The paper also describes a prototype tool that implements our technique along with a set of empirical studies that use the tool for evaluation of the technique. The empirical studies show that, for the subject we studied, the technique can be effective in helping a user locate faults in a program.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Jones, James A. and Harrold, Mary Jean and Stasko, John},
	month = may,
	year = {2002},
	pages = {467--477},
}

@inproceedings{eagan_technical_2001,
	title = {Technical {Note}: {Visually} {Encoding} {Program} {Test} {Information} to {Find} {Faults} in {Software}},
	shorttitle = {Technical note},
	doi = {10.1109/INFVIS.2001.963277},
	booktitle = {{IEEE} {Symposium} on {Information} {Visualization}, 2001. {INFOVIS} 2001.},
	author = {Eagan, J. and Harrold, Mary Jean and Jones, James A. and Stasko, John},
	month = oct,
	year = {2001},
	note = {ISSN: 1522-404X},
	keywords = {Software systems, Debugging, Software maintenance, Software testing, System testing, Educational institutions, Encoding, Programming profession, Software quality, User interfaces},
	pages = {33--36},
	file = {IEEE Xplore Abstract Record:files/80/963277.html:text/html},
}

@inproceedings{harrold_regression_2001,
	address = {New York, NY, USA},
	series = {{OOPSLA} '01},
	title = {Regression {Test} {Selection} for {Java} {Software}},
	isbn = {978-1-58113-335-6},
	url = {https://doi.org/10.1145/504282.504305},
	doi = {10.1145/504282.504305},
	abstract = {Regression testing is applied to modified software to provide confidence that the changed parts behave as intended and that the unchanged parts have not been adversely affected by the modifications. To reduce the cost of regression testing, test cases are selected from the test suite that was used to test the original version of the software---this process is called regression test selection. A safe regression-test-selection algorithm selects every test case in the test suite that may reveal a fault in the modified software. Safe regression-test-selection technique that, based on the use of a suitable representation, handles the features of the Java language. Unlike other safe regression test selection techniques, the presented technique also handles incomplete programs. The technique can thus be safely applied in the (very common) case of Java software that uses external libraries of components; the analysis of the external code is note required for the technique to select test cases for such software. The paper also describes RETEST, a regression-test-selection algorithm can be effective in reducing the size of the test suite.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 16th {ACM} {SIGPLAN} conference on {Object}-oriented programming, systems, languages, and applications},
	publisher = {Association for Computing Machinery},
	author = {Harrold, Mary Jean and Jones, James A. and Li, Tongyu and Liang, Donglin and Orso, Alessandro and Pennings, Maikel and Sinha, Saurabh and Spoon, S. Alexander and Gujarathi, Ashish},
	month = oct,
	year = {2001},
	pages = {312--326},
}

@inproceedings{jones_visualization_2003,
	title = {Visualization for {Fault} {Localization}},
	url = {https://www.semanticscholar.org/paper/Visualization-for-Fault-Localization-Jones-Harrold/a512b04457cd02779508b60662fd5c632abab78d},
	abstract = {Software errors significantly impact software productivity and quality. Attempts to reduce the number of delivered faults are estimated to consume between 50\% and 80\% of the development and maintenance effort [4]. Debugging is one of the most time-consuming, and thus expensive, tasks required to reduce the number of delivered faults in a program. Because software debugging is so expensive, researchers have investigated techniques and tools to assist developers with these tasks (e.g., [1, 3, 7]). However, these tools often do not scale to large programs or they require extensive manual intervention. This lack of effective tools hinders the development and maintenance process. Studies show that locating the errors is the most difficult and time-consuming component of the debugging process (e.g., [8]). Pan and Spafford observed that developers consistently perform four tasks when attempting to locate the errors in a program: (1) identify statements involved in failures; (2) select suspicious statements that might contain faults; (3) hypothesize about suspicious faults; and (4) restore program variables to a specific state [6]. A source-code debugger can help with the first task: a developer runs the program, one line at a time, with a test case that caused it to fail, and during this execution, the developer can inspect the results produced by the execution of each statement in the program. Information about incorrect results at a statement can help a developer locate the source of the problem. Stepping through large programs one statement at a time, however, and inspecting the results of the execution can be very time consuming. Thus, developers often try to localize the problem area by working backwards from the location of the failure (e.g., computing a slice). By considering all statements that affect the location in which an incorrect value occurred, a developer may be able to locate the cause of the failure. A source-code debugger can also help a developer with the fourth task: a developer can set breakpoints, reset the program state, and execute the program with the modified state. This process may help a developer concentrate on smaller regions of code that may be the cause of the failure. Although these tools can help developers locate faults, there are several aspects of this fault-localization that can be improved. First, even with source-code debuggers and slicers, the manual process of identifying the location of the faults can be very time consuming. A technique that can automate, or partially automate, the process can provide significant savings. Second, because these tools lead developers to focus their attention locally instead of providing a global view of the software, interacting faults are difficult to detect. An approach that provides the developer with a global view of the software, while still giving access to the local view, can provide a developer with more useful information. Third, the tools use results of only one execution of the program instead of using information provided by many executions of the program; such information is typically an artifact of the testing process. A tool that provides information about many executions of the program lets the developer understand more complex relationships in the system. To address these problems, we have begun to develop visualization techniques and tools that can improve developers’ ability to locate faults. Techniques and heuristics such as those described above may help a developer focus on areas of the program where faults may occur. However, with large programs and multiple faults, the huge amount of data produced by such an approach, if reported in a textual form, may be difficult},
	urldate = {2023-09-05},
	author = {Jones, James A. and Harrold, Mary Jean and Stasko, John},
	year = {2003},
	file = {Full Text PDF:files/85/Jones et al. - 2003 - Visualization for Fault Localization.pdf:application/pdf},
}

@article{harrold_empirical_1998,
	title = {Empirical {Studies} of {Control} {Dependence} {Graph} {Size} {forC} {Programs}},
	volume = {3},
	issn = {1382-3256},
	url = {https://doi.org/10.1023/A:1008088300124},
	doi = {10.1023/A:1008088300124},
	abstract = {Many tools and techniques for performing software engineering tasks require control-dependence information, represented in the form of control-dependence graphs. Worst-case analysis of these graphs has shown that their size may be quadratic in the number of statements in the procedure that they represent. Despite this result, two empirical studies suggest that in practice, the relationship between control-dependence graph size and program size is linear. These studies, however, were performed on a relatively small number of Fortran procedures, all of which were derived from numerical methods programs. To further investigate control-dependence size, we implemented tools for constructing the two most popular types of control-dependence graphs, and ran our tools on over 3000 C functions extracted from a wide range of source programs. Our results support the earlier conclusions about control-dependence graph size.},
	number = {2},
	urldate = {2023-09-05},
	journal = {Empirical Software Engineering},
	author = {Harrold, Mary Jean and Jones, James A. and Rothermel, Gregg},
	month = aug,
	year = {1998},
	keywords = {software engineering, program analysis, static analysis, control dependence},
	pages = {203--211},
}

@article{servant_automatic_2015,
	title = {Automatic {Execution}-{Based} {Bug} {Assignment}},
	abstract = {In this article, we present a developer-to-bug assignment technique that uses information from a bug-revealing execution to inform automatic recommendation. In contrast with many existing automatic bug-assignment techniques, this execution-based technique addresses situations in which a bug-report is not immediately available. Predominantly, the existing techniques use the natural-language descriptions within bug reports to model the expertise required to investigate, find, and fix bugs. However, a bug report is not always available before the bug needs to be assigned to a developer --- e.g. when the bug is exposed by a test-case failure or when a developer needs to be assigned to the task of writing a bug report. In such situations, these existing techniques are incompatible for early and automatic bug assignment. To address this gap, we present WHOSEFAULT, a technique that models the expertise required to fix a bug by analyzing bug executions. As a result, WHOSEFAULT is not sensitive to the presence, quality, and consistency of human-written bug reports. This article presents a comparative evaluation in terms of effectiveness and efficiency between WHOSEFAULT and a set of representative bug-assignment techniques that use a bug report to model the expertise required to fix the bug. In this evaluation, we observed that WHOSEFAULT was, on average, more effective across subjects than the most effective techniques that require a bug report. However, WHOSEFAULT was less efficient than the most efficient techniques that require a bug report. Nonetheless, its median execution time was less than one minute, and in the worst case was two minutes. The results of this study showed that, in the absence of a bug report, an automatic developer recommendation could still be obtained equally or more effectively as with bug-assignment techniques that require a bug report, but moderately less efficiently.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Servant, Francisco and Jones, James A.},
	month = jan,
	year = {2015},
	pages = {1--17},
}

@article{digiuseppe_automatic_2015,
	title = {Automatic {Natual}-{Lanaguage} {Fault} {Diagnoses}},
	abstract = {While debugging software, developers seek understanding of several aspects of the faults that cause failures, such as, where in the code are the faults, how are such locations executed to cause failure, and what is the nature of the fault and the features that cause failure. Despite a large body of research for providing automation for the first two tasks (i.e., “where” and “how”), very little work has been conducted in helping to assist in the last question of “what” --- that is, for describing the nature of the fault. This work represents a first step in this direction: applying automated inferencing methods and information-retrieval techniques to mine the source code in order to specify topics (i.e., words or concepts) that can indicate the nature of faults and their failures. In practice, we find that these words are indeed indicative of faults, and moreover, that the combinations of those words provide fuller descriptions of faults than the individual words, by leveraging knowledge and cognitive abilities of the developers to recognize topical patterns in the combinations of terms. We describe scenarios in which developers may benefit from these automatically produced fault diagnoses. Additionally, we present the design of our technique, its implementation, and two evaluations that include both (1) an automated, fully-reproducible experiment and (2) a controlled user-study experiment. The first, automated, experiment evaluates the accuracy of the diagnoses for 75 real faults from the ASPECTJ software system, and the second, human-study, experiment evaluates the comprehension of the diagnosed faults for 12 software developers. The results of these experiments demonstrate that the technique provided fast and early fault diagnoses that were both accurate and useful for human comprehension of the nature of the faults.},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {DiGiuseppe, Nicholas and Jones, James A.},
	month = jan,
	year = {2015},
	pages = {1--36},
}

@inproceedings{deng_specification_2011,
	title = {Specification and {Runtime} {Verification} of {API} {Constraints} on {Interacting} {Objects}.},
	abstract = {Most applications need to invoke some Application Progi'ainming Interfaces (APIs), e.g. JDK (Java Development Kit) API. When invoking those APIs, applications must follow some API constraints. Violation of these constraints will lead to some severe program defects. To detect this kind of defects, lots of static and dynamic approaches are explored, using formally described API constraints. While most existing approaches explore API constraints on a single object, this paper focuses on API constraints on interacting objects (COIOs). We proposed a novel specification language LACOIO (Language for API Constraint on Interacting Objects) which can cover all state-of-the-art reported API COIOs. We implemented a LACOIO compiler which can automatically generate monitoring code for runtime verification from API constraints that are specified using LACOIO and implemented the runtime verification framework. We evaluated the proposed approach by comparing it with Tracematch - a popular constraint specification language.},
	author = {Deng, Fang and Liu, Haiwen and Shao, Jin and Wang, Qianxiang},
	month = jan,
	year = {2011},
	pages = {101--106},
}

@inproceedings{shao_lazy_2010,
	title = {Lazy {Runtime} {Verification} for {Constraints} on {Interacting} {Objects}},
	doi = {10.1109/APSEC.2010.36},
	author = {Shao, Jin and Deng, Fang and Liu, Haiwen and Wang, Qianxiang and Mei, Hong},
	month = nov,
	year = {2010},
	pages = {242--251},
}

@article{harris_security_nodate,
	title = {Security {Testing} of {Session} {Initiation} {Protocol} {Implementations}},
	volume = {1},
	url = {https://www.magiran.com/paper/1205209/security-testing-of-session-initiation-protocol-implementations?lang=en},
	abstract = {The mechanisms which enable the vast majority of computer attacks are based on design and programming errors in networked applications. The growing use of voice over IP (VOIP) phone technology makes these phone applications potential targets. We present a tool to perform security testing of VOIP applications to identify security vulnerabilities which can be exploited by an attacker. Session Initiation Protocol (SIP) is the widespread standard for establishing and ending VOIP communication sessions. Our tool generates an input sequence for a SIP phone which is designed to reveal security vulnerabilities in the SIP phone application. The input sequence includes SIP messages and external graphical user interface (GUI) events which might contribute to triggering a vulnerability. The input sequence is generated to perform a random walk through the state space of the protocol. The generation of external GUI events is critical to testing a stateful protocol such as SIP because GUI interaction is required to explore a significant portion of the state space. We have used our security testing tool to identify a previously unknown vulnerability in an existing open source SIP phone.},
	language = {fa},
	number = {2},
	urldate = {2023-09-12},
	journal = {International Journal of Information Security},
	author = {Harris, Ian G. and Alrahem, Thoulfekar and Chen, Alex and DiGiuseppe, Nicholas and Gee, Jeffery and Hsiao, Shang-Pin and Mattox, Sean and Park, Tarjoon},
	pages = {91--103},
	file = {Snapshot:files/95/security-testing-of-session-initiation-protocol-implementations.html:text/html},
}

@inproceedings{shao_model-driven_2007,
	title = {A {Model}-{Driven} {Software} {Monitoring} and {Analyzing} {Framework}},
	abstract = {On many J2EE application servers, applications and components are
    deployed in a static way on particular servers. Applications and components
    could not be moved after being deployed and started. In a large J2EE cluster,
    the runtime state of one node changes dynamically. The distribution of
    applications needs to be reconfigured among several nodes for many reasons.
    Based on application server PKUAS, we implement an dynamic migration method
    of EJB component to support the runtime redistribution of EJB.},
	booktitle = {Proceeding of the 6th {National} {Software} and {Application} {Conference}},
	author = {Shao, Jin and Deng, Fang and Wang, Qianxiang},
	month = sep,
	year = {2007},
}

@article{digiuseppe_real_2007,
	title = {Real {Genders} {Choose} {Fantasy} {Characters}: {Class} {Choice} in {World} of {Warcraft}},
	copyright = {Copyright (c)},
	issn = {1396-0466},
	shorttitle = {Real genders choose fantasy characters},
	url = {https://firstmonday.org/ojs/index.php/fm/article/view/1831},
	doi = {10.5210/fm.v12i5.1831},
	abstract = {Gender stereotypes inflect discussions about character choice in World of Warcraft (WoW), an online multiplayer video game. We interviewed 47 players, 33 males and 14 females, about why they chose their characters. We found the stereotypes contained a grain of truth as stereotypes often do, but that they mask interesting dimensions of gender in character choice.},
	language = {en},
	urldate = {2023-09-12},
	journal = {First Monday},
	author = {DiGiuseppe, Nicholas and Nardi, Bonnie},
	month = may,
	year = {2007},
}
