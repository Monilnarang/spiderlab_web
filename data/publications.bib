
@article{noauthor_notitle_nodate,
}

@inproceedings{feng_empirical_2018,
	title = {An {Empirical} {Study} on {Software} {Failure} {Classification} with {Multi}-label and {Problem}-{Transformation} {Techniques}},
	doi = {10.1109/ICST.2018.00039},
	abstract = {Classification techniques have been used in software-engineering research to perform tasks such as categorizing software executions. Traditionally, existing work has proposed single-label failure classification techniques, in which the training and subsequent executions are labeled with a singular fault attribution. Although such approaches have received substantial attention in research on automated software engineering, in reality, recent work shows that the assumption of such a single attribution is often unrealistic: in practice, the inherent characteristics of software behavior, such as multiple faults that contribute to failures and fault interactions, may negatively influence the effectiveness of these techniques. To relax this unrealistic assumption, in the machine learning field, researchers have proposed new approaches for multi-label classification. However, the effectiveness and efficiency of such approaches varies widely based upon application domains. In this paper, we empirically investigate the performance of these new approaches on the failure classification task under different application settings. We conducted experiments using eight classification techniques on five subject programs with more than 8,000 faulty versions to investigate how each such technique accounts for the intricacies of software behavior. Our experimental results show that multi-label techniques provide improved accuracy over single-label. We also evaluated the efficiency of the training and prediction phases of each technique, and offer guidance as to the applicability for each technique for different usage contexts.},
	booktitle = {2018 {IEEE} 11th {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Feng, Yang and Jones, James and Chen, Zhenyu and Fang, Chunrong},
	month = apr,
	year = {2018},
	keywords = {Computer crashes, Empirical Study, Failure Classification, Machine learning, Maintenance engineering, multi-label classification, Software, Task analysis, Training, Training data},
	pages = {320--330},
	file = {IEEE Xplore Abstract Record:files/4/8367059.html:text/html},
}

@inproceedings{liu_generating_2018,
	title = {Generating descriptions for screenshots to assist crowdsourced testing},
	doi = {10.1109/SANER.2018.8330246},
	abstract = {Crowdsourced software testing has been shown to be capable of detecting many bugs and simulating real usage scenarios. As such, it is popular in mobile-application testing. However in mobile testing, test reports often consist of only some screenshots and short text descriptions. Inspecting and under-standing the overwhelming number of mobile crowdsourced test reports becomes a time-consuming but inevitable task. The paucity and potential inaccuracy of textual information and the well-defined screenshots of activity views within mobile applications motivate us to propose a novel technique to assist developers in understanding crowdsourced test reports by automatically describing the screenshots. To reach this goal, in this paper, we propose a fully automatic technique to generate descriptive words for the well-defined screenshots. We employ the test reports written by professional testers to build up language models. We use the computer-vision technique, namely Spatial Pyramid Matching (SPM), to measure similarities and extract features from the screenshot images. The experimental results, based on more than 1000 test reports from 4 industrial crowdsourced projects, show that our proposed technique is promising for developers to better understand the mobile crowdsourced test reports.},
	booktitle = {2018 {IEEE} 25th {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Liu, Di and Zhang, Xiaofang and Feng, Yang and Jones, James A.},
	month = mar,
	year = {2018},
	keywords = {Buildings, Computer bugs, Feature extraction, Mobile applications, Task analysis, Testing, Training},
	pages = {492--496},
	file = {IEEE Xplore Abstract Record:files/6/8330246.html:text/html},
}
